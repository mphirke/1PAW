{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The_Annotated_Transformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mphirke/1PAW/blob/master/Week1/The_Annotated_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIX4VivhwQSg",
        "colab_type": "text"
      },
      "source": [
        "## 1 PAW Week 1 (22 Sept - 29 Sept, 2019)\n",
        "Reimplementation of paper [\"Attention is all you need\" ](https://arxiv.org/abs/1706.03762) using (mostly) [The Annotated Transformer ](https://github.com/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb) , [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)  and the PyTorch docs.\n",
        "\n",
        "# The Architecture \n",
        "\n",
        "![this](https://www.dropbox.com/s/m1sqqlie8y05hz9/arch.png?dl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_ZKM_Kdg0aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVCVIv9xjfkK",
        "colab_type": "text"
      },
      "source": [
        "The Generator essential gives us the final probabilities after the inputs is passed through all the rest of our architecture. \n",
        "\n",
        "![Linear + Softmax](https://www.dropbox.com/s/ntpqyov5r3thw2h/generator.png?dl=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilqj3xDMxcAa",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_wtvTD7i--i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR-7orNepM3Q",
        "colab_type": "text"
      },
      "source": [
        "## Understanding the Generator  - \n",
        "\n",
        "### Understanding nn.Linear\n",
        "nn.Linear(input_features, outputs_features)\n",
        "\n",
        "$y = xW^T + b$\n",
        "\n",
        "**y** = output\n",
        "\n",
        "**W** = weights\n",
        "\n",
        "**x** = input\n",
        "\n",
        "\n",
        "**b** = bias\n",
        "\n",
        "The weights and the bias are randomly initliazed.\n",
        "\n",
        "Essentially, we are creating a linear equation of the form $y= xW^T + b$ , where our input is the d_model and the output is the vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raLGUIPVnYDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "4dd71a9a-c41a-4270-a78f-7eb528f702b9"
      },
      "source": [
        "m = nn.Linear(3,1)\n",
        "inp = torch.tensor([[1.0, -1.0, 10]])\n",
        "out = m(inp)\n",
        "print(\"The output is\", out)\n",
        "print(m.weight)\n",
        "print(m.bias)   "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The output is tensor([[2.7858]], grad_fn=<AddmmBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0718,  0.2321,  0.3375]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2856], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzW0cZiqrbo",
        "colab_type": "text"
      },
      "source": [
        "## Understanding log_softmax\n",
        "\n",
        "from [PyTorch documentation](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.log_softmax)\n",
        "\n",
        "> Applies a softmax followed by a logarithm. While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.\n",
        "\n",
        "Softmax is given by - \n",
        "\n",
        "# $ Softmax(x) =  \\frac {e ^x } {\\sum e ^ (x_j) } $\n",
        "\n",
        "Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3NS-_X9qcRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "e65c517d-ab08-4dd3-c36b-05b92e318f31"
      },
      "source": [
        "print(F.softmax(torch.tensor([1, 2.0, 3 ,4 ]), dim=-1))\n",
        "print(F.log_softmax(torch.tensor([1,2.0,3,4])))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0321, 0.0871, 0.2369, 0.6439])\n",
            "tensor([-3.4402, -2.4402, -1.4402, -0.4402])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fHUIy-Ov25Z",
        "colab_type": "text"
      },
      "source": [
        "Reimplementing log_softmax in numpy to compare results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEDS7Y3EsP4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(inps):\n",
        "    sum_denominator = np.sum(np.exp(inps))\n",
        "    softmaxes = np.array(np.exp(inps))/sum_denominator\n",
        "    return softmaxes\n",
        "\n",
        "def log_sm(inps):\n",
        "    softmaxes = softmax(inps)\n",
        "    return np.log(softmaxes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYAlATKotxtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "61168f6b-20da-4b04-eac8-1392b03b5ff6"
      },
      "source": [
        "print(softmax([1, 2.0, 3, 4]))\n",
        "log_sm([1, 2.0, 3, 4])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0320586  0.08714432 0.23688282 0.64391426]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.4401897, -2.4401897, -1.4401897, -0.4401897])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbfGjrAKv8O3",
        "colab_type": "text"
      },
      "source": [
        "We get the same thing.\n",
        "So, confirmed that log_softmax returns \n",
        "# $ log (\\frac {e ^x } {\\sum e ^ (x_j) } )$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzXD-SGxyjn",
        "colab_type": "text"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "Consists of a stack of six encoders.\n",
        "Why 6 encoders? No special reason. It's open to experimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMpqAV_CyQem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "    # _ means blank. Essentially, it deepcopies the module for N times, hence building a stack of N same layers.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2JZrQj8dM9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    #Essentially follows an Encode > Decode architecture\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        return self.decode(self.encoder(src,src_mask), src_mask, tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed, src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}